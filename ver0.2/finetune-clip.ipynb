{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Caption</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8ae451edcd8ebf697f8763ece249115813149c55733bf8...</td>\n",
       "      <td>C√¥ ·∫•y tr√™n m·∫°ng vs c√¥ ·∫•y ngo√†i ƒë·ªùi =)))</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35370ffd6c791d6f8c4ab3dd4363ed468fab41e4824ee9...</td>\n",
       "      <td>Ng∆∞·ªùi t√¢m linh giao ti·∫øp v·ªõi ng∆∞·ªùi th·ª±c t·∫ø :)))</td>\n",
       "      <td>not-sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316fdd1477725b9fb1a55015ac06b68b92b50bd4303e08...</td>\n",
       "      <td>H√¨nh nh∆∞ TrƒÉng h√¥m nay ƒë·∫πp qu√° m·ªçi ng∆∞·ªùi ·∫°! üòÉ ...</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8a0f34e0e30e4e5cfb306933c1d25fa801a5da78646b59...</td>\n",
       "      <td>M·ªåI NG∆Ø·ªúI NGHƒ® SAO V·ªÄ PH√ÅT BI·ªÇU C·ª¶A SHARK VI·ªÜT...</td>\n",
       "      <td>not-sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e517a5e95d1065886a7c815e82fe254381d4f9f4b244d4...</td>\n",
       "      <td>2 tay hai n√†ng ch·ª© vi·ªác g√¨ ph·∫£i l·ªá hai h√†ng</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Image  \\\n",
       "0  8ae451edcd8ebf697f8763ece249115813149c55733bf8...   \n",
       "1  35370ffd6c791d6f8c4ab3dd4363ed468fab41e4824ee9...   \n",
       "2  316fdd1477725b9fb1a55015ac06b68b92b50bd4303e08...   \n",
       "3  8a0f34e0e30e4e5cfb306933c1d25fa801a5da78646b59...   \n",
       "4  e517a5e95d1065886a7c815e82fe254381d4f9f4b244d4...   \n",
       "\n",
       "                                             Caption        Label  \n",
       "0            C√¥ ·∫•y tr√™n m·∫°ng vs c√¥ ·∫•y ngo√†i ƒë·ªùi =)))      sarcasm  \n",
       "1    Ng∆∞·ªùi t√¢m linh giao ti·∫øp v·ªõi ng∆∞·ªùi th·ª±c t·∫ø :)))  not-sarcasm  \n",
       "2  H√¨nh nh∆∞ TrƒÉng h√¥m nay ƒë·∫πp qu√° m·ªçi ng∆∞·ªùi ·∫°! üòÉ ...      sarcasm  \n",
       "3  M·ªåI NG∆Ø·ªúI NGHƒ® SAO V·ªÄ PH√ÅT BI·ªÇU C·ª¶A SHARK VI·ªÜT...  not-sarcasm  \n",
       "4        2 tay hai n√†ng ch·ª© vi·ªác g√¨ ph·∫£i l·ªá hai h√†ng      sarcasm  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import clip\n",
    "import torch\n",
    "import tqdm\n",
    "import json\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('../data/datasets/train-sarcasm-dataset.csv', encoding='utf-8')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of train data: 7563\n",
      "Lenght of val data: 3242\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.7 * len(data))\n",
    "val_size = len(data) - train_size\n",
    "train_dataset, val_dataset = random_split(data, [train_size, val_size])\n",
    "\n",
    "print(\"Lenght of train data: {}\".format(len(train_dataset)))\n",
    "print(\"Lenght of val data: {}\".format(len(val_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, subcategories):\n",
    "        self.data = data\n",
    "        self.subcategories = subcategories\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image_path = \"../data/images/train-images/{}\".format(item['Image'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")  \n",
    "        subcategory = item['Label']\n",
    "        label = self.subcategories.index(subcategory) if subcategory in self.subcategories else -1\n",
    "        return self.transform(image), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "labels = [\"sarcasm\", \"not-sarcasm\"]\n",
    "text_inputs = processor(text=[f\"{label}.\" for label in labels], return_tensors=\"pt\", padding=True).to(device)\n",
    "subcategories = list(data['Label'].unique())\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(data.to_dict(orient='records'), test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(ImageDataset(train_dataset, subcategories), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(ImageDataset(val_dataset, subcategories), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape: torch.Size([32, 3, 224, 224])\n",
      "Labels Shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(\"Image Shape:\", images.shape)\n",
    "    print(\"Labels Shape:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CLIPFineTuner(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes):\n",
    "        super(CLIPFineTuner, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.fc = nn.Linear(clip_model.config.projection_dim, num_classes)\n",
    "\n",
    "    def forward(self, images, input_ids):\n",
    "        outputs = self.clip_model(pixel_values=images, input_ids=input_ids)\n",
    "        logits = self.fc(outputs.image_embeds)\n",
    "        return logits\n",
    "\n",
    "num_classes = len(subcategories)\n",
    "model_ft = CLIPFineTuner(model, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_ft.fc.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/16, Loss: 0.0000:   0%|          | 0/271 [00:00<?, ?it/s]c:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:480: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Epoch 1/16, Loss: 0.6898: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [01:37<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/16], Loss: 0.6898\n",
      "Validation Accuracy: 58.861638130495145%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/16, Loss: 0.6763: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [01:37<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/16], Loss: 0.6763\n",
      "Validation Accuracy: 59.18556223970384%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/16, Loss: 0.6669: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [01:37<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/16], Loss: 0.6669\n",
      "Validation Accuracy: 61.26793151318834%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/16, Loss: 0.6596: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [01:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/16], Loss: 0.6596\n",
      "Validation Accuracy: 63.39657565941694%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/16, Loss: 0.6532: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [01:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/16], Loss: 0.6532\n",
      "Validation Accuracy: 63.48912540490514%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/16, Loss: 0.6476: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [01:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/16], Loss: 0.6476\n",
      "Validation Accuracy: 64.09069875057844%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/16, Loss: 0.6426: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [01:36<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/16], Loss: 0.6426\n",
      "Validation Accuracy: 64.64599722350763%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/16, Loss: 0.6388: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [01:37<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/16], Loss: 0.6388\n",
      "Validation Accuracy: 65.06247107820454%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/16, Loss: 0.6355: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [01:36<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/16], Loss: 0.6355\n",
      "Validation Accuracy: 65.20129569643683%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/16, Loss: 0.6319: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [01:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/16], Loss: 0.6319\n",
      "Validation Accuracy: 65.20129569643683%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/16, Loss: 0.6287: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [01:37<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/16], Loss: 0.6287\n",
      "Validation Accuracy: 65.29384544192503%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/16, Loss: 0.6265: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [01:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/16], Loss: 0.6265\n",
      "Validation Accuracy: 65.61776955113373%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/16, Loss: 0.6235: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [01:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/16], Loss: 0.6235\n",
      "Validation Accuracy: 65.47894493290143%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/16, Loss: 0.6226: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [01:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/16], Loss: 0.6226\n",
      "Validation Accuracy: 65.38639518741323%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/16, Loss: 0.6199: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [01:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/16], Loss: 0.6199\n",
      "Validation Accuracy: 65.47894493290143%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/16, Loss: 0.6180: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [01:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/16], Loss: 0.6180\n",
      "Validation Accuracy: 65.75659416936604%\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_ft.train() \n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}, Loss: 0.0000\")\n",
    "\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)  \n",
    "        optimizer.zero_grad()  \n",
    "        input_ids = text_inputs.input_ids.repeat(images.size(0), 1)  \n",
    "        outputs = model_ft(images, input_ids) \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "\n",
    "        running_loss += loss.item() \n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\") \n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}') \n",
    "\n",
    "    model_ft.eval()  \n",
    "    correct = 0 \n",
    "    total = 0 \n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device) \n",
    "            input_ids = text_inputs.input_ids.repeat(images.size(0), 1) \n",
    "            outputs = model_ft(images, input_ids)  \n",
    "            _, predicted = torch.max(outputs.data, 1)  \n",
    "            total += labels.size(0) \n",
    "            correct += (predicted == labels).sum().item()  \n",
    "    print(f'Validation Accuracy: {100 * correct / total}%') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), '../data/models/clip-vit-base-patch32-finetuned.pth')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
